{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T22:43:11.600167",
     "start_time": "2016-11-02T14:42:37.135Z"
    },
    "collapsed": false,
    "hide_input": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "module RNN\n",
    "\n",
    "o = Dict()\n",
    "o[:embed] = 256\n",
    "o[:epochs] = 3\n",
    "o[:batchsize] = 128\n",
    "o[:seqlength] = 100\n",
    "o[:decay] = 0.9\n",
    "o[:lr] = 1.0\n",
    "o[:gclip] = 3.0\n",
    "o[:winit] = 0.3\n",
    "o[:gcheck] = 0\n",
    "o[:seed] = -1\n",
    "o[:atype] = Array{Float32}\n",
    "o[:fast] = true\n",
    "\n",
    "o[:Lmin] = 1 # minimum number of hidden layers\n",
    "o[:Lmax] = 5 # maximum number of hidden layers\n",
    "o[:Nmax] = 300 # maximum number of neurons\n",
    "o[:Nmin] = 50  # minimum number of neurons\n",
    "o[:Nstep] = 50 # N = Nmin:Nstep:Nmax\n",
    "o[:lr_min] = 0.01 # minimum learning rate\n",
    "o[:lr_max] = 0.5 # maximum learning rate\n",
    "o[:maxevals] = 10 # maximum evaluations of hyperopt\n",
    "############################################################\n",
    "\n",
    "using Knet,AutoGrad,JLD,Hyperopt\n",
    "export Knet,JLD\n",
    "\n",
    "initialize(data, o) = \n",
    "initweights(o[:atype], o[:hidden], data, o[:embed], o[:winit])\n",
    "\n",
    "function minibatch(x, y, batch_size)\n",
    "    n, dx, dy = size(x,2), size(x,1), size(y,1)\n",
    "    nbatch = div(n, batch_size)\n",
    "    data = [(zeros(batch_size,dx),zeros(batch_size,dy)) for i=1:nbatch ] \n",
    "    cidx = 0\n",
    "    for idx = 1:size(x,2)           # safest way to iterate over utf-8 text\n",
    "        idata = 1 + idx % nbatch\n",
    "        row = 1 + div(idx, nbatch)\n",
    "        row > batch_size && break\n",
    "        data[idata][1][row,:] = x[:,idx]\n",
    "        data[idata][2][row,:] = y[:,idx]\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function train!(model, data, o)\n",
    "    o[:bestfile] = tempname()*\".jld\"\n",
    "    save(o[:bestfile], \"model\", model)\n",
    "    s0 = initstate(o[:atype], o[:hidden], o[:batchsize])\n",
    "    lr = o[:lr]\n",
    "    if o[:fast]\n",
    "        @time (for epoch=1:o[:epochs]\n",
    "               train1(model, copy(s0), data[1]; slen=o[:seqlength], lr=lr, gclip=o[:gclip])\n",
    "               end; Knet.cudaDeviceSynchronize())\n",
    "        save(o[:bestfile], \"model\", model)\n",
    "        acc = accuracy(model,copy(s0),data[1])   \n",
    "        return 1-acc, o[:bestfile]\n",
    "    end    \n",
    "    losses = map(d->loss(model,copy(s0),d), data)\n",
    "    println((:epoch,0,:loss,losses...))\n",
    "    devset = ifelse(length(data) > 1, 2, 1)\n",
    "    devlast = devbest = losses[devset]\n",
    "    for epoch=1:o[:epochs]\n",
    "        @time train1(model, copy(s0), data[1]; slen=o[:seqlength], lr=lr, gclip=o[:gclip])\n",
    "        @time losses = map(d->loss(model,copy(s0),d), data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "        if o[:gcheck] > 0\n",
    "            gradcheck(loss, model, copy(s0), data[1], 1:o[:seqlength]; gcheck=o[:gcheck])\n",
    "        end\n",
    "        devloss = losses[devset]\n",
    "        if devloss < devbest\n",
    "            devbest = devloss\n",
    "            info(\"Saving best model to $(o[:bestfile])\")\n",
    "            save(o[:bestfile], \"model\", model)\n",
    "        end\n",
    "        if devloss > devlast\n",
    "            lr *= o[:decay]\n",
    "            info(\"New learning rate: $lr\")\n",
    "        end\n",
    "        devlast = devloss\n",
    "    end\n",
    "    acc = accuracy(model,copy(s0),data[devset])  \n",
    "    1-acc, o[:bestfile]\n",
    "end    \n",
    "\n",
    "\n",
    "# sequence[t]: input token at time t\n",
    "# state is modified in place\n",
    "function train1(param, state, sequence; slen=100, lr=1.0, gclip=0.0)\n",
    "    for t = 1:slen:length(sequence)-slen\n",
    "        range = t:t+slen-1\n",
    "        gloss = lossgradient(param, state, sequence, range)\n",
    "        gscale = lr\n",
    "        if gclip > 0\n",
    "            gnorm = sqrt(mapreduce(sumabs2, +, 0, gloss))\n",
    "            if gnorm > gclip\n",
    "                gscale *= gclip / gnorm\n",
    "            end\n",
    "        end\n",
    "        gnorm = sqrt(mapreduce(sumabs2, +, 0, gloss))\n",
    "        for k in 1:length(param)\n",
    "            # param[k] -= gscale * gloss[k]\n",
    "            Knet.axpy!(-gscale, gloss[k], param[k])\n",
    "        end\n",
    "        isa(state,Vector{Any}) || error(\"State should not be Boxed.\")\n",
    "        # The following is needed in case AutoGrad boxes state values during gradient calculation\n",
    "        for i = 1:length(state)\n",
    "            state[i] = AutoGrad.getval(state[i])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# param[2k-1,2k]: weight and bias for the k'th lstm layer\n",
    "# param[end-2]: embedding matrix\n",
    "# param[end-1,end]: weight and bias for final prediction\n",
    "function initweights(atype, hidden, data, embed, winit)\n",
    "    dx, dy = size(data[1][1][1],2), size(data[1][1][2],2)\n",
    "    param = Array(Any, 2*length(hidden)+3)\n",
    "    input = embed\n",
    "    for k = 1:length(hidden)\n",
    "        param[2k-1] = winit*randn(input+hidden[k], 4*hidden[k])\n",
    "        param[2k]   = zeros(1, 4*hidden[k])\n",
    "        param[2k][1:hidden[k]] = 1 # forget gate bias\n",
    "        input = hidden[k]\n",
    "    end\n",
    "    param[end-2] = winit*randn(dx,embed)\n",
    "    param[end-1] = winit*randn(hidden[end],dy)\n",
    "    param[end] = zeros(1,dy)\n",
    "    return map(p->convert(atype,p), param)\n",
    "end\n",
    "\n",
    "# state[2k-1,2k]: hidden and cell for the k'th lstm layer\n",
    "function initstate(atype, hidden, batchsize)\n",
    "    state = Array(Any, 2*length(hidden))\n",
    "    for k = 1:length(hidden)\n",
    "        state[2k-1] = zeros(batchsize,hidden[k])\n",
    "        state[2k] = zeros(batchsize,hidden[k])\n",
    "    end\n",
    "    return map(s->convert(atype,s), state)\n",
    "end\n",
    "\n",
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm(gates[:,1:hsize])\n",
    "    ingate  = sigm(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh(cell)\n",
    "    return (hidden,cell)\n",
    "end\n",
    "\n",
    "# s[2k-1,2k]: hidden and cell for the k'th lstm layer\n",
    "# w[2k-1,2k]: weight and bias for k'th lstm layer\n",
    "# w[end-2]: embedding matrix\n",
    "# w[end-1,end]: weight and bias for final prediction\n",
    "# state is modified in place\n",
    "function predict(w, s, x)\n",
    "    x = x * w[end-2]\n",
    "    for i = 1:2:length(s)\n",
    "        (s[i],s[i+1]) = lstm(w[i],w[i+1],s[i],s[i+1],x)\n",
    "        x = s[i]\n",
    "    end\n",
    "    return x * w[end-1] .+ w[end]\n",
    "end\n",
    "\n",
    "# sequence[t]: input token at time t\n",
    "# state is modified in place\n",
    "function loss(param,state,sequence,range=1:length(sequence)-1)\n",
    "    total = 0.0; count = 0\n",
    "    atype = typeof(AutoGrad.getval(param[1]))\n",
    "    for t in range\n",
    "        input = convert(atype,sequence[t][1])\n",
    "        ypred = predict(param,state,input)\n",
    "        ynorm = logp(ypred,2) # ypred .- log(sum(exp(ypred),2))\n",
    "        ygold = convert(atype,sequence[t][2])\n",
    "        total += sum(ygold .* ynorm)\n",
    "        count += size(ygold,1)\n",
    "    end\n",
    "    return -total / count\n",
    "end\n",
    "\n",
    "function accuracy(param,state,sequence,range=1:length(sequence)-1)\n",
    "    total = 0.0; count = 0\n",
    "    atype = typeof(AutoGrad.getval(param[1]))\n",
    "    for t in range\n",
    "        input = convert(atype,sequence[t][1])\n",
    "        ypred = predict(param,state,input)\n",
    "        ygold = convert(atype,sequence[t][2])\n",
    "        total += sum(ygold .* (ypred .== maximum(ypred,2)))\n",
    "        count += size(ygold,1)\n",
    "    end\n",
    "    return total / count\n",
    "end\n",
    "\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "# To be able to load/save KnetArrays:\n",
    "if Pkg.installed(\"JLD\") != nothing\n",
    "    import JLD: writeas, readas\n",
    "    type KnetJLD; a::Array; end\n",
    "    writeas(c::KnetArray) = KnetJLD(Array(c))\n",
    "    readas(d::KnetJLD) = KnetArray(d.a)\n",
    "end\n",
    "#####################################################\n",
    "\n",
    "function splitdata(x, y)\n",
    "    n1, n2 = size(x)\n",
    "    xtrn = x[:,1:4*n2รท5]\n",
    "    xtst = x[:,4*n2รท5+1:n2]\n",
    "    ytrn = y[:,1:4*n2รท5]\n",
    "    ytst = y[:,4*n2รท5+1:n2]\n",
    "    return  xtrn, ytrn, xtst, ytst\n",
    "end \n",
    "\n",
    "function label2vec(l)\n",
    "    !haskey(o,:lu) && (o[:lu] = unique(l))\n",
    "    lu = o[:lu] # unique label\n",
    "    eltype(lu)<:Number && (lu = sort(lu))\n",
    "    v = zeros(length(lu),length(l))\n",
    "    for j = 1:size(v,2)\n",
    "        i = find(lu .== l[j])[1]\n",
    "        v[i,j] = 1\n",
    "    end\n",
    "    v\n",
    "end\n",
    "\n",
    "function vec2label(v)\n",
    "    lu = o[:lu]\n",
    "    l = Array{eltype(lu)}(1,size(v,2))\n",
    "    for j = 1:size(v,2)\n",
    "        i = findmax(v[:,j])[2]\n",
    "        l[1,j] = lu[i]\n",
    "    end\n",
    "    l\n",
    "end\n",
    "\n",
    "function preprocess(x,y)\n",
    "    o[:M] = M = maximum(x,2)\n",
    "    o[:m] = m = minimum(x,2)\n",
    "    x = (x.- m)./(M .- m .+ 1e-20)\n",
    "    y = label2vec(y)\n",
    "    return x,y\n",
    "end\n",
    "\n",
    "function generate(param, vocab, nchar)\n",
    "    state = initstate(o[:atype], o[:hidden], 1)\n",
    "    index_to_char = Array(Char, length(vocab))\n",
    "    for (k,v) in vocab; index_to_char[v] = k; end\n",
    "    input = oftype(param[1], zeros(1,length(vocab)))\n",
    "    index = 1\n",
    "    for t in 1:nchar\n",
    "        ypred = predict(param,state,input)\n",
    "        input[index] = 0\n",
    "        index = sample(exp(logp(ypred)))\n",
    "        print(index_to_char[index])\n",
    "        input[index] = 1\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end\n",
    "\n",
    "function objective(args)\n",
    "    h, lr = args\n",
    "    h = Int.(collect(h))\n",
    "    o[:hidden],o[:lr] = h,lr\n",
    "        \n",
    "    model = initialize(o[:data], o)\n",
    "    loss, file = train!(model, o[:data], o)\n",
    "    \n",
    "    @printf(\"\\nnlayer=%d,layers=%s,lr=%6.4f,loss=%6.4f\\n\",\n",
    "    length(h),h,lr,loss)\n",
    "    \n",
    "    Knet.knetgc()\n",
    "    return Dict(\"loss\" => loss, \"status\" => STATUS_OK, \"model_file\" => file)\n",
    "end\n",
    "\n",
    "function main(x, y; gpu=false)  \n",
    "    gpu == true && (o[:atype] = KnetArray{Float32})\n",
    "    x, y = preprocess(x, y)\n",
    "    xtrn, ytrn, xtst, ytst = splitdata(x, y)\n",
    "    dtrn = minibatch(xtrn, ytrn, o[:batchsize])\n",
    "    dtst = minibatch(xtst, ytst, o[:batchsize])\n",
    "    o[:data] = [dtrn, dtst]\n",
    "\n",
    "    trials = Trials()\n",
    "    hps = [] # hyper-parameters\n",
    "    opt = [] # option  \n",
    "    \n",
    "    for l = o[:Lmin]:o[:Lmax]\n",
    "        push!(hps,quniform(\"h$l\",o[:Nmin],o[:Nmax],o[:Nstep]))\n",
    "        push!(opt,(hps...))\n",
    "    end\n",
    "    \n",
    "    best_args = fmin(objective,\n",
    "    space=[choice(\"hidden\",opt),uniform(\"lr\", o[:lr_min], o[:lr_max])],\n",
    "        algo=TPESUGGEST,\n",
    "        maxevals=o[:maxevals],\n",
    "        trials = trials)\n",
    "    \n",
    "    best_loss, best_ind = findmin(losses(trials))    \n",
    "    best_model_file = trials[\"results\"][best_ind][\"model_file\"]\n",
    "    best_model = load(best_model_file,\"model\")\n",
    "    best_model, best_args, best_loss\n",
    "end\n",
    "\n",
    "function predict1(model, state, x)\n",
    "    x = (x.- o[:m])./(o[:M] .- o[:m] .+ 1e-20)\n",
    "    preds = Array{Any}(size(x,2))\n",
    "    probs = zeros(size(x,2))  \n",
    "    for t = 1:size(x,2)\n",
    "        y = Array(predict(model, state, o[:atype](x[:,t])'))\n",
    "        probs[t] = maximum(exp(y)./sum(exp(y)))\n",
    "        preds[t] = vec2label(y)[1]\n",
    "    end\n",
    "    return preds,probs,state\n",
    "end\n",
    "function predict(model, x)\n",
    "    state = initstate(o[:atype], o[:hidden], 1)\n",
    "    x = (x.- o[:m])./(o[:M] .- o[:m] .+ 1e-20)\n",
    "    preds = Array{Any}(size(x,2))\n",
    "    probs = zeros(size(x,2))  \n",
    "    for t = 1:size(x,2)\n",
    "        y = Array(predict(model, state, o[:atype](x[:,t])'))'\n",
    "        probs[t] = maximum(exp(y)./sum(exp(y)))\n",
    "        preds[t] = vec2label(y)\n",
    "    end\n",
    "    return preds,probs,state\n",
    "end \n",
    "\n",
    "end # end of module RNN\n",
    "\n",
    "# text = readstring(\"/home/rluser/.julia/v0.5/Knet/data/100.txt\")[1:30000]\n",
    "# vocab = Dict{Char,Int}()\n",
    "# for t in text, c in t; get!(vocab, c, 1+length(vocab)); end\n",
    "# x = RNN.label2vec(text[1:end-1])\n",
    "# y = text[2:end]\n",
    "# RNN.save(\"/tmp/data.jld\",\"x\",x,\"y\",y,\"vocab\",vocab,\"lu\",RNN.o[:lu])\n",
    "\n",
    "# RNN.@load(\"/tmp/data.jld\")\n",
    "# RNN.o[:lu] = lu\n",
    "# RNN.vec2label(x)\n",
    "\n",
    "\n",
    "RNN.@load(\"/tmp/data.jld\")\n",
    "RNN.o[:lu] = lu\n",
    "\n",
    "RNN.o[:maxevals] = 1\n",
    "RNN.o[:Nmax] = RNN.o[:Nmin] = 256\n",
    "RNN.o[:Nstep] = RNN.o[:Lmin] = RNN.o[:Lmax] = 1\n",
    "RNN.o[:fast] = false\n",
    "\n",
    "import Utils\n",
    "Utils.@save_output begin\n",
    "    best_model, best_args, best_loss = RNN.main(x,y)\n",
    "    RNN.generate(best_model, vocab, 100)\n",
    "    preds, probs, state = RNN.predict(best_model, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
