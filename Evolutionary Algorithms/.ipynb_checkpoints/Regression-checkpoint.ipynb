{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "module Reg\n",
    "##################################################################\n",
    "module MLP\n",
    "\n",
    "const Lmax = 5 # maximum number of hidden layers\n",
    "const Nmax = 300 # maximum number of neurons\n",
    "const Nmin = 50  # minimum number of neurons\n",
    "const epochs = 100\n",
    "const maxevals = 1\n",
    "const lr_min = 0.01# minimum learning rate\n",
    "const lr_max = 0.5\n",
    "\n",
    "using Knet, Hyperopt\n",
    "\n",
    "function splitdata(x, y)\n",
    "    n1, n2 = size(x)\n",
    "    xtrn = x[:,1:4*n2รท5]\n",
    "    xtst = x[:,4*n2รท5+1:n2]\n",
    "    ytrn = y[:,1:4*n2รท5]\n",
    "    ytst = y[:,4*n2รท5+1:n2]\n",
    "    return  xtrn, ytrn, xtst, ytst\n",
    "end \n",
    "\n",
    "\n",
    "function preprocess(x,y)\n",
    "    global Mx, mx, My, my       \n",
    "    Mx = maximum(x,2); mx = minimum(x,2)\n",
    "    My = maximum(y,2); my = minimum(y,2)\n",
    "    x = (x.- mx)./(Mx .- mx .+ 1e-20)\n",
    "    y = (y.- my)./(My .- my .+ 1e-20)\n",
    "    return x,y\n",
    "end\n",
    "\n",
    "function predict(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*x .+ w[i+1]\n",
    "        if i<length(w)-1\n",
    "            x = relu(x) # max(0,x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function loss(w,x,y)\n",
    "    ypred = predict(w,x)\n",
    "    sumabs2(y - ypred) / size(y,2)\n",
    "end\n",
    "\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function train(w, dtrn; lr=.5, epochs=10)\n",
    "    for epoch=1:epochs\n",
    "        for (x,y) in dtrn\n",
    "            g = lossgradient(w, x, y)\n",
    "            for i in 1:length(w)\n",
    "                w[i] -= lr * g[i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end\n",
    "\n",
    "function err(w,dtrn)\n",
    "    cost = ninstance = 0.0\n",
    "    for (x, ygold) in dtrn\n",
    "        cost += loss(w,x,ygold)\n",
    "        ninstance += 1.0\n",
    "    end\n",
    "    cost/ninstance\n",
    "end\n",
    "\n",
    "\n",
    "function weights(data,h...; winit=0.1)\n",
    "    x0,y0 = data[1]\n",
    "    atype = typeof(MLP.dtrn[1][1])\n",
    "    w = Any[]\n",
    "    x = size(x0,1)\n",
    "    for y in [h..., size(y0,1)]\n",
    "        push!(w, convert(atype, winit*randn(y,x)))\n",
    "        push!(w, convert(atype, zeros(y, 1)))\n",
    "        x = y\n",
    "    end\n",
    "    return w  \n",
    "end\n",
    "\n",
    "function minibatch(x, y, batchsize; atype=Array{Float32})\n",
    "    x = atype(x); y = atype(y)\n",
    "    data = Any[]\n",
    "    for i=1:batchsize:size(x,2)\n",
    "        j=min(i+batchsize-1,size(x,2))\n",
    "        push!(data, (x[:,i:j], y[:,i:j]))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function objective(args)\n",
    "    h, lr = args\n",
    "    h = Int.(collect(h))\n",
    "    \n",
    "    global dtrn\n",
    "    global dtst\n",
    "    w = weights(dtrn,h...)\n",
    "\n",
    "    loss = 1000\n",
    "    for i=1:epochs\n",
    "        train(w, dtrn; lr=lr, epochs=1)\n",
    "        cost = err(w, dtst)\n",
    "        if cost < loss\n",
    "            loss = cost\n",
    "        end\n",
    "    end\n",
    "    @printf(\"\\nnlayer=%d,layers=%s,lr=%6.4f,loss=%6.4f\\n\",length(h),h,lr,loss)\n",
    "\n",
    "    return Dict(\"loss\" => loss, \"status\" => STATUS_OK, \"model\" => w)\n",
    "end\n",
    "\n",
    "export main\n",
    "global dtrn, dtst, Mx, mx, My, my, atype\n",
    "function main(x, y; batchsize=50, gpu=false)\n",
    "    global dtrn, dtst, atype\n",
    "    x, y = preprocess(x, y)\n",
    "    xtrn, ytrn, xtst, ytst = splitdata(x, y)\n",
    "    gpu == false ? (atype = Array{Float32}) : (atype = KnetArray{Float32})\n",
    "    dtrn = minibatch(xtrn, ytrn, batchsize; atype=atype)\n",
    "    dtst = minibatch(xtst, ytst, batchsize; atype=atype)\n",
    "\n",
    "    trials = Trials()\n",
    "    hps = [] # hyper-parameters\n",
    "    opt = [] # option  \n",
    "    \n",
    "    for l = 1:Lmax\n",
    "        push!(hps,quniform(\"h$l\",Nmin,Nmax,50))\n",
    "        push!(opt,(hps...))\n",
    "    end\n",
    "    \n",
    "    best_args = fmin(objective,\n",
    "    space=[choice(\"hidden\",opt),uniform(\"lr\", lr_min, lr_max)],\n",
    "        algo=TPESUGGEST,\n",
    "        maxevals=maxevals,\n",
    "        trials = trials)\n",
    "    \n",
    "    best_loss, best_ind = findmin(losses(trials))    \n",
    "    best_model = trials[\"results\"][best_ind][\"model\"]\n",
    "    \n",
    "    best_model, best_args, best_loss\n",
    "end\n",
    "\n",
    "end # end of module MLP\n",
    "\n",
    "##################################################################\n",
    "export main\n",
    "function main(x, y; o...)\n",
    "    net, net_args, net_loss = MLP.main(x, y; o...)\n",
    "end\n",
    "\n",
    "export predict\n",
    "function predict(model::Array{Any}, x)\n",
    "    x = (x.- MLP.mx)./(MLP.Mx .- MLP.mx .+ 1e-20)\n",
    "    y = Array(MLP.predict(model, MLP.atype(x)))\n",
    "    y = MLP.my .+ (MLP.My .- MLP.my).*y  \n",
    "end \n",
    "\n",
    "\n",
    "end# end of module Reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include(Pkg.dir(\"Knet/examples/housing.jl\"))\n",
    "# atype = Array{Float32}\n",
    "# (xtrn,ytrn,xtst,ytst) = map(x->convert(atype,x), Housing.loaddata())\n",
    "# import Reg\n",
    "# net, net_args, net_loss = Reg.main(xtrn, ytrn; gpu=true);\n",
    "# net_preds = Reg.predict(net, xtrn)\n",
    "\n",
    "# using Gadfly\n",
    "# plot(x=ytrn', y=net_preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
