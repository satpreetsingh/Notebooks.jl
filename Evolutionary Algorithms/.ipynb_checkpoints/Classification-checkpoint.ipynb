{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "module Class\n",
    "##################################################################\n",
    "module MLP\n",
    "\n",
    "const Lmax = 5 # maximum number of hidden layers\n",
    "const Nmax = 300 # maximum number of neurons\n",
    "const Nmin = 50  # minimum number of neurons\n",
    "const epochs = 10\n",
    "const maxevals = 10\n",
    "const lr_min = 0.01# minimum learning rate\n",
    "const lr_max = 0.5\n",
    "\n",
    "using Knet, Hyperopt\n",
    "\n",
    "\n",
    "function splitdata(x, y)\n",
    "    n1, n2 = size(x)\n",
    "    xtrn = x[:,1:4*n2÷5]\n",
    "    xtst = x[:,4*n2÷5+1:n2]\n",
    "    ytrn = y[:,1:4*n2÷5]\n",
    "    ytst = y[:,4*n2÷5+1:n2]\n",
    "    return  xtrn, ytrn, xtst, ytst\n",
    "end \n",
    "\n",
    "function label2vec(l)\n",
    "    global lu\n",
    "    lu = unique(l) # unique label\n",
    "    eltype(lu)<:Number && (lu = sort(lu))\n",
    "    v = zeros(length(lu),length(l))\n",
    "    for j = 1:size(v,2)\n",
    "        i = find(lu .== l[j])[1]\n",
    "        v[i,j] = 1\n",
    "    end\n",
    "    v\n",
    "end\n",
    "\n",
    "function vec2label(v)\n",
    "    global lu\n",
    "    l = Array{eltype(lu)}(1,size(v,2))\n",
    "    for j = 1:size(v,2)\n",
    "        i = findmax(v[:,j])[2]\n",
    "        l[1,j] = lu[i]\n",
    "    end\n",
    "    l\n",
    "end\n",
    "\n",
    "function preprocess(x,y)\n",
    "    global M, m\n",
    "    M = maximum(x,2)\n",
    "    m = minimum(x,2)\n",
    "    x = (x.- m)./(M .- m .+ 1e-20)\n",
    "    y = label2vec(y)\n",
    "    return x,y\n",
    "end\n",
    "\n",
    "function predict(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*x .+ w[i+1]\n",
    "        if i<length(w)-1\n",
    "            x = relu(x) # max(0,x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function loss(w,x,ygold)\n",
    "    ypred = predict(w,x)\n",
    "    ynorm = logp(ypred,1) # ypred .- log(sum(exp(ypred),1))\n",
    "    -sum(ygold .* ynorm) / size(ygold,2)\n",
    "end\n",
    "\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function train(w, dtrn; lr=.5, epochs=10)\n",
    "    for epoch=1:epochs\n",
    "        for (x,y) in dtrn\n",
    "            g = lossgradient(w, x, y)\n",
    "            for i in 1:length(w)\n",
    "                w[i] -= lr * g[i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end\n",
    "\n",
    "function err(w,dtrn)\n",
    "    cost = 0.0\n",
    "    for (x, ygold) in dtrn\n",
    "        cost += loss(w,x,ygold)\n",
    "    end\n",
    "    cost\n",
    "end\n",
    "\n",
    "function accuracy(w, dtst)\n",
    "    ncorrect = ninstance = 0\n",
    "    for (x, ygold) in dtst\n",
    "        ypred = predict(w, x)\n",
    "        ncorrect += sum(ygold .* (ypred .== maximum(ypred,1)))\n",
    "        ninstance += size(ygold,2)\n",
    "    end\n",
    "    return ncorrect/ninstance\n",
    "end\n",
    "\n",
    "function weights(data,h...; winit=0.1)\n",
    "    x0,y0 = data[1]\n",
    "    atype = typeof(MLP.dtrn[1][1])\n",
    "    w = Any[]\n",
    "    x = size(x0,1)\n",
    "    for y in [h..., size(y0,1)]\n",
    "        push!(w, convert(atype, winit*randn(y,x)))\n",
    "        push!(w, convert(atype, zeros(y, 1)))\n",
    "        x = y\n",
    "    end\n",
    "    return w  \n",
    "end\n",
    "\n",
    "function minibatch(x, y, batchsize; atype=Array{Float32})\n",
    "    x = atype(x); y = atype(y)\n",
    "    data = Any[]\n",
    "    for i=1:batchsize:size(x,2)\n",
    "        j=min(i+batchsize-1,size(x,2))\n",
    "        push!(data, (x[:,i:j], y[:,i:j]))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function objective(args)\n",
    "    h, lr = args\n",
    "    h = Int.(collect(h))\n",
    "    \n",
    "    global dtrn\n",
    "    global dtst\n",
    "    w = weights(dtrn,h...)\n",
    "\n",
    "    loss = 1000\n",
    "    for i=1:epochs\n",
    "        train(w, dtrn; lr=lr, epochs=1)\n",
    "        err = 1 - accuracy(w, dtst)\n",
    "        if err < loss\n",
    "            loss = err\n",
    "        end\n",
    "    end\n",
    "    @printf(\"\\nnlayer=%d,layers=%s,lr=%6.4f,loss=%6.4f\\n\",\n",
    "    length(h),h,lr,loss)\n",
    "\n",
    "    return Dict(\"loss\" => loss, \"status\" => STATUS_OK, \"model\" => w)\n",
    "end\n",
    "\n",
    "export main\n",
    "global dtrn, dtst, M, m, lu, atype\n",
    "function main(x, y; batchsize=50, gpu=false)\n",
    "    global dtrn, dtst, atype\n",
    "    x, y = preprocess(x, y)\n",
    "    xtrn, ytrn, xtst, ytst = splitdata(x, y)\n",
    "    gpu == false ? (atype = Array{Float32}) : (atype = KnetArray{Float32})\n",
    "    dtrn = minibatch(xtrn, ytrn, batchsize; atype=atype)\n",
    "    dtst = minibatch(xtst, ytst, batchsize; atype=atype)\n",
    "\n",
    "    trials = Trials()\n",
    "    hps = [] # hyper-parameters\n",
    "    opt = [] # option  \n",
    "    \n",
    "    for l = 1:Lmax\n",
    "        push!(hps,quniform(\"h$l\",Nmin,Nmax,50))\n",
    "        push!(opt,(hps...))\n",
    "    end\n",
    "    \n",
    "    best_args = fmin(objective,\n",
    "    space=[choice(\"hidden\",opt),uniform(\"lr\", lr_min, lr_max)],\n",
    "        algo=TPESUGGEST,\n",
    "        maxevals=maxevals,\n",
    "        trials = trials)\n",
    "    \n",
    "    best_loss, best_ind = findmin(losses(trials))    \n",
    "    best_model = trials[\"results\"][best_ind][\"model\"]\n",
    "    \n",
    "    best_model, best_args, best_loss\n",
    "end\n",
    "\n",
    "end # end of module MLP\n",
    "\n",
    "##################################################################\n",
    "module Boost\n",
    "\n",
    "const max_depth_max = 100\n",
    "const num_round_max = 10\n",
    "const η_min = 0.1\n",
    "const η_max = 1.0\n",
    "const maxevals = 50\n",
    "\n",
    "using XGBoost, Hyperopt\n",
    "\n",
    "function splitdata(x, y)\n",
    "    n1, n2 = size(x)\n",
    "    xtrn = x[1:4*n1÷5, :]\n",
    "    xtst = x[4*n1÷5+1:n1, :]\n",
    "    ytrn = y[1:4*n1÷5]\n",
    "    ytst = y[4*n1÷5+1:n1]\n",
    "    return  xtrn, ytrn, xtst, ytst\n",
    "end \n",
    "\n",
    "function label2int(l)\n",
    "    global lu\n",
    "    lu = unique(l) # unique label\n",
    "    eltype(lu)<:Number && (lu = sort(lu))\n",
    "    I = zeros(Int,l)\n",
    "    for i in eachindex(I)\n",
    "        I[i] = find(lu .== l[i])[1] - 1\n",
    "    end\n",
    "    I\n",
    "end\n",
    "\n",
    "function int2label(I)\n",
    "    global lu\n",
    "    I = Int.(I)\n",
    "    l = zeros(I)\n",
    "    for i in eachindex(I)\n",
    "        l[i] = lu[I[i]+1]\n",
    "    end\n",
    "    l\n",
    "end\n",
    "    \n",
    "function preprocess(x,y)  \n",
    "    y = vec(y)\n",
    "    y = label2int(y)\n",
    "    return x,y\n",
    "end\n",
    "\n",
    "\n",
    "function objective(args)\n",
    "    global xtrn, ytrn, xtst, ytst, nclass\n",
    "    num_round,max_depth,η = args\n",
    "    num_round = Int(num_round)\n",
    "    max_depth = Int(max_depth)\n",
    "    \n",
    "    param = Dict(\"max_depth\"=>max_depth,\"eta\"=>η, \"num_class\"=>nclass,\n",
    "    \"silent\"=>2, \"objective\"=>\"multi:softmax\")\n",
    "    bst = xgboost(xtrn, num_round, label=ytrn,param=param)\n",
    "    preds = predict(bst, xtst)\n",
    "    loss = mean(preds .!= ytst)\n",
    "   \n",
    "    @printf(\"\\nnum_round=%d,max_depth=%d,η=%6.4f,loss=%6.4f\\n\",\n",
    "    num_round,max_depth,η,loss)\n",
    "    return Dict(\"loss\" => loss, \"status\" => STATUS_OK,\"model\" => bst)\n",
    "end\n",
    "\n",
    "global xtrn, ytrn, xtst, ytst, nclass\n",
    "\n",
    "export main\n",
    "function main(x, y; batchsize=50, gpu=false)\n",
    "    global xtrn, ytrn, xtst, ytst, nclass\n",
    "    x, y = preprocess(x, y)\n",
    "    xtrn, ytrn, xtst, ytst = splitdata(x, y)\n",
    "    nclass = length(unique(y))\n",
    "    \n",
    "    trials = Trials()   \n",
    "    best_args = fmin(objective,\n",
    "    space=[quniform(\"num_round\",1, num_round_max,1),\n",
    "        quniform(\"max_depth\",2, max_depth_max,1),\n",
    "        uniform(\"eta\", η_min, η_max)],\n",
    "        algo=TPESUGGEST,\n",
    "        maxevals=maxevals,\n",
    "        trials = trials)\n",
    "   \n",
    "    best_loss, best_ind = findmin(losses(trials))    \n",
    "    best_model = trials[\"results\"][best_ind][\"model\"]\n",
    "    \n",
    "    best_model, best_args, best_loss\n",
    "end\n",
    "\n",
    "end # end of module Boost\n",
    "\n",
    "##################################################################\n",
    "export main\n",
    "function main(x, y; o...)\n",
    "    net, net_args, net_loss = MLP.main(x, y; o...)\n",
    "    bst, bst_args, bst_loss = Boost.main(x, y; o...)\n",
    "    net, net_args, net_loss, bst, bst_args, bst_loss\n",
    "end\n",
    "\n",
    "export predict\n",
    "function predict(model::Boost.XGBoost.Booster, x)\n",
    "    x = x'\n",
    "    I = Boost.predict(model, x) # integer\n",
    "    l = Boost.int2label(I) # label\n",
    "end \n",
    "function predict(model::Array{Any}, x)\n",
    "    x = (x.- MLP.m)./(MLP.M .- MLP.m .+ 1e-20)\n",
    "    y = Array(MLP.predict(model, MLP.atype(x)))\n",
    "    prob = exp(y)./sum(exp(y),1)\n",
    "    preds = MLP.vec2label(y)\n",
    "    return preds,prob\n",
    "end \n",
    "\n",
    "\n",
    "end# end of module Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import Class,MNIST\n",
    "# xtrn, ytrn = MNIST.traindata()\n",
    "# xtst, ytst = MNIST.testdata()\n",
    "# x = [xtrn xtst]\n",
    "# y = [ytrn;ytst] + 10\n",
    "# # x = x[1:10,1:20]\n",
    "# # y = y[1:20]\n",
    "# net, net_args, net_loss, bst, bst_args, bst_loss = Class.main(x,y; gpu=true)\n",
    "\n",
    "# net_preds, net_prob = Class.predict(net, x)\n",
    "# bst_preds = Class.predict(bst, x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
