{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T11:14:43.816689",
     "start_time": "2016-11-02T03:14:53.256Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module CharLM\n",
      "WARNING: Method definition writeas(Knet.KnetArray) in module CharLM at In[5]:212 overwritten in module CharLM at In[12]:212.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharLM"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module CharLM\n",
    "\n",
    "using Knet,AutoGrad,JLD\n",
    "export Knet,JLD\n",
    "\n",
    "function initialize(text, o)\n",
    "    if o[:loadfile]==nothing\n",
    "        vocab = Dict{Char,Int}()\n",
    "        for t in text, c in t; get!(vocab, c, 1+length(vocab)); end\n",
    "        model = CharLM.initweights(o[:atype], o[:hidden], length(vocab), o[:embed], o[:winit])\n",
    "    else\n",
    "        info(\"Loading model from $(o[:loadfile])\")\n",
    "        vocab = JLD.load(o[:loadfile], \"vocab\") \n",
    "        for t in text, c in t; haskey(vocab, c) || error(\"Unknown char $c\"); end\n",
    "        model = map(p->convert(o[:atype],p), JLD.load(o[:loadfile], \"model\"))\n",
    "    end\n",
    "    model, vocab\n",
    "end\n",
    "\n",
    "function train!(model, text, vocab, o)\n",
    "    s0 = initstate(o[:atype], o[:hidden], o[:batchsize])\n",
    "    data = map(t->minibatch(t, vocab, o[:batchsize]), text)\n",
    "    lr = o[:lr]\n",
    "    if o[:fast]\n",
    "        @time (for epoch=1:o[:epochs]\n",
    "               train1(model, copy(s0), data[1]; slen=o[:seqlength], lr=lr, gclip=o[:gclip])\n",
    "               end; Knet.cudaDeviceSynchronize())\n",
    "        return\n",
    "    end    \n",
    "    losses = map(d->loss(model,copy(s0),d), data)\n",
    "    println((:epoch,0,:loss,losses...))\n",
    "    devset = ifelse(length(data) > 1, 2, 1)\n",
    "    devlast = devbest = losses[devset]\n",
    "    for epoch=1:o[:epochs]\n",
    "        @time train1(model, copy(s0), data[1]; slen=o[:seqlength], lr=lr, gclip=o[:gclip])\n",
    "        @time losses = map(d->loss(model,copy(s0),d), data)\n",
    "        println((:epoch,epoch,:loss,losses...))\n",
    "        if o[:gcheck] > 0\n",
    "            gradcheck(loss, model, copy(s0), data[1], 1:o[:seqlength]; gcheck=o[:gcheck])\n",
    "        end\n",
    "        devloss = losses[devset]\n",
    "        if devloss < devbest\n",
    "            devbest = devloss\n",
    "            if o[:bestfile] != nothing\n",
    "                info(\"Saving best model to $(o[:bestfile])\")\n",
    "                save(o[:bestfile], \"model\", model, \"vocab\", vocab)\n",
    "            end\n",
    "        end\n",
    "        if devloss > devlast\n",
    "            lr *= o[:decay]\n",
    "            info(\"New learning rate: $lr\")\n",
    "        end\n",
    "        devlast = devloss\n",
    "    end\n",
    "end    \n",
    "\n",
    "\n",
    "# sequence[t]: input token at time t\n",
    "# state is modified in place\n",
    "function train1(param, state, sequence; slen=100, lr=1.0, gclip=0.0)\n",
    "    for t = 1:slen:length(sequence)-slen\n",
    "        range = t:t+slen-1\n",
    "        gloss = lossgradient(param, state, sequence, range)\n",
    "        gscale = lr\n",
    "        if gclip > 0\n",
    "            gnorm = sqrt(mapreduce(sumabs2, +, 0, gloss))\n",
    "            if gnorm > gclip\n",
    "                gscale *= gclip / gnorm\n",
    "            end\n",
    "        end\n",
    "        gnorm = sqrt(mapreduce(sumabs2, +, 0, gloss))\n",
    "        for k in 1:length(param)\n",
    "            # param[k] -= gscale * gloss[k]\n",
    "            Knet.axpy!(-gscale, gloss[k], param[k])\n",
    "        end\n",
    "        isa(state,Vector{Any}) || error(\"State should not be Boxed.\")\n",
    "        # The following is needed in case AutoGrad boxes state values during gradient calculation\n",
    "        for i = 1:length(state)\n",
    "            state[i] = AutoGrad.getval(state[i])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# param[2k-1,2k]: weight and bias for the k'th lstm layer\n",
    "# param[end-2]: embedding matrix\n",
    "# param[end-1,end]: weight and bias for final prediction\n",
    "function initweights(atype, hidden, vocab, embed, winit)\n",
    "    param = Array(Any, 2*length(hidden)+3)\n",
    "    input = embed\n",
    "    for k = 1:length(hidden)\n",
    "        param[2k-1] = winit*randn(input+hidden[k], 4*hidden[k])\n",
    "        param[2k]   = zeros(1, 4*hidden[k])\n",
    "        param[2k][1:hidden[k]] = 1 # forget gate bias\n",
    "        input = hidden[k]\n",
    "    end\n",
    "    param[end-2] = winit*randn(vocab,embed)\n",
    "    param[end-1] = winit*randn(hidden[end],vocab)\n",
    "    param[end] = zeros(1,vocab)\n",
    "    return map(p->convert(atype,p), param)\n",
    "end\n",
    "\n",
    "# state[2k-1,2k]: hidden and cell for the k'th lstm layer\n",
    "function initstate(atype, hidden, batchsize)\n",
    "    state = Array(Any, 2*length(hidden))\n",
    "    for k = 1:length(hidden)\n",
    "        state[2k-1] = zeros(batchsize,hidden[k])\n",
    "        state[2k] = zeros(batchsize,hidden[k])\n",
    "    end\n",
    "    return map(s->convert(atype,s), state)\n",
    "end\n",
    "\n",
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm(gates[:,1:hsize])\n",
    "    ingate  = sigm(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh(cell)\n",
    "    return (hidden,cell)\n",
    "end\n",
    "\n",
    "# s[2k-1,2k]: hidden and cell for the k'th lstm layer\n",
    "# w[2k-1,2k]: weight and bias for k'th lstm layer\n",
    "# w[end-2]: embedding matrix\n",
    "# w[end-1,end]: weight and bias for final prediction\n",
    "# state is modified in place\n",
    "function predict(w, s, x)\n",
    "    x = x * w[end-2]\n",
    "    for i = 1:2:length(s)\n",
    "        (s[i],s[i+1]) = lstm(w[i],w[i+1],s[i],s[i+1],x)\n",
    "        x = s[i]\n",
    "    end\n",
    "    return x * w[end-1] .+ w[end]\n",
    "end\n",
    "\n",
    "# sequence[t]: input token at time t\n",
    "# state is modified in place\n",
    "function loss(param,state,sequence,range=1:length(sequence)-1)\n",
    "    total = 0.0; count = 0\n",
    "    atype = typeof(AutoGrad.getval(param[1]))\n",
    "    input = convert(atype,sequence[first(range)])\n",
    "    for t in range\n",
    "        ypred = predict(param,state,input)\n",
    "        ynorm = logp(ypred,2) # ypred .- log(sum(exp(ypred),2))\n",
    "        ygold = convert(atype,sequence[t+1])\n",
    "        total += sum(ygold .* ynorm)\n",
    "        count += size(ygold,1)\n",
    "        input = ygold\n",
    "    end\n",
    "    return -total / count\n",
    "end\n",
    "\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function generate(param, state, vocab, nchar)\n",
    "    index_to_char = Array(Char, length(vocab))\n",
    "    for (k,v) in vocab; index_to_char[v] = k; end\n",
    "    input = oftype(param[1], zeros(1,length(vocab)))\n",
    "    index = 1\n",
    "    for t in 1:nchar\n",
    "        ypred = predict(param,state,input)\n",
    "        input[index] = 0\n",
    "        index = sample(exp(logp(ypred)))\n",
    "        print(index_to_char[index])\n",
    "        input[index] = 1\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "\n",
    "function sample(p)\n",
    "    p = convert(Array,p)\n",
    "    r = rand()\n",
    "    for c = 1:length(p)\n",
    "        r -= p[c]\n",
    "        r < 0 && return c\n",
    "    end\n",
    "end\n",
    "\n",
    "function shakespeare()\n",
    "    file = Pkg.dir(\"Knet\",\"data\",\"100.txt\")\n",
    "    if !isfile(file)\n",
    "        info(\"Downloading 'The Complete Works of William Shakespeare'\")\n",
    "        url = \"http://www.gutenberg.org/files/100/100.txt\"\n",
    "        download(url,file)\n",
    "    end\n",
    "    return file\n",
    "end\n",
    "\n",
    "function minibatch(chars, char_to_index, batch_size)\n",
    "    nbatch = div(length(chars), batch_size)\n",
    "    vocab_size = length(char_to_index)\n",
    "    data = [ falses(batch_size, vocab_size) for i=1:nbatch ] # using BitArrays\n",
    "    cidx = 0\n",
    "    for c in chars            # safest way to iterate over utf-8 text\n",
    "        idata = 1 + cidx % nbatch\n",
    "        row = 1 + div(cidx, nbatch)\n",
    "        row > batch_size && break\n",
    "        col = char_to_index[c]\n",
    "        data[idata][row,col] = 1\n",
    "        cidx += 1\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "# To be able to load/save KnetArrays:\n",
    "if Pkg.installed(\"JLD\") != nothing\n",
    "    import JLD: writeas, readas\n",
    "    type KnetJLD; a::Array; end\n",
    "    writeas(c::KnetArray) = KnetJLD(Array(c))\n",
    "    readas(d::KnetJLD) = KnetArray(d.a)\n",
    "end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T11:15:11.667894",
     "start_time": "2016-11-02T03:15:21.175Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = Dict()\n",
    "o[:datafiles] = [\"/home/rluser/.julia/v0.5/Knet/data/101.txt\"]\n",
    "o[:loadfile] = nothing\n",
    "o[:savefile] = \"/tmp/CharLM-last.jld\"\n",
    "o[:bestfile] = \"/tmp/CharLM-best.jld\"\n",
    "o[:generate] = 0\n",
    "o[:hidden] = [256]\n",
    "o[:embed] = 256\n",
    "o[:epochs] = 3\n",
    "o[:batchsize] = 128\n",
    "o[:seqlength] = 50 # 100\n",
    "o[:decay] = 0.9\n",
    "o[:lr] = 1.0\n",
    "o[:gclip] = 3.0\n",
    "o[:winit] = 0.3\n",
    "o[:gcheck] = 0\n",
    "o[:seed] = -1\n",
    "o[:atype] = Knet.KnetArray{Float32}\n",
    "o[:fast] = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T11:15:16.451935",
     "start_time": "2016-11-02T03:15:21.995Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.541751 seconds (1.84 M allocations: 86.737 MB)\n"
     ]
    }
   ],
   "source": [
    "text = map(readstring,o[:datafiles])\n",
    "model, vocab = CharLM.initialize(text, o)\n",
    "CharLM.train!(model, text, vocab, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T11:15:18.751925",
     "start_time": "2016-11-02T03:15:27.655Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n",
      "1T1TTTJeql~~9~~~oJ#\n"
     ]
    }
   ],
   "source": [
    "state = CharLM.initstate(o[:atype], o[:hidden], 1)\n",
    "CharLM.generate(model, state, vocab, 100)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
